Critique of the Current XAI Tachycardia Detection System
Overview of the Data Pipeline and Model

The tachycardia detection system is built on the MIT-BIH Arrhythmia Database, which contains 48 ECG recordings (~30 minutes each) from 47 patients
ojs.stanford.edu
. The system uses an explainable AI (XAI) approach to identify episodes of tachycardia. While the foundational pipeline demonstrates the potential of machine learning in ECG analysis, several critical shortcomings currently prevent it from being ready for real-world clinical use. These issues span the data and labeling strategy, model training choices, evaluation methodology, and explainability aspects. Below is a detailed critique of each component, followed by recommendations to enhance the system's generalizability, sensitivity, and reliability.

Data and Labeling Shortcomings
Class Imbalance and SMOTE Limitations

Imbalanced Tachycardia Data: Tachycardia (especially pathological tachyarrhythmias) instances are extremely scarce in the MIT-BIH dataset relative to normal rhythms. The database has a highly skewed class distribution, with normal sinus beats dominating and abnormal rhythms comprising only a small fraction
ojs.stanford.edu
. As a result, a model can achieve high overall accuracy by mostly predicting the majority (normal) class, yet fail to detect the minority (tachy) events ‚Äì a serious recall gap.

 

Use of SMOTE: The project attempts to address imbalance using SMOTE (Synthetic Minority Over-sampling Technique). While SMOTE can oversample minority classes by creating synthetic examples, it has notably limited effectiveness for ECG time-series. SMOTE generates new samples by interpolating between existing minority examples, **but it does not introduce new physiological variations seen in real arrhythmias
ojs.stanford.edu
. In other words, the synthetic tachycardia beats created may be too similar to existing ones, causing the model to overfit these few patterns rather than learn the true diversity of tachyarrhythmias. This is problematic because real-world ECGs vary widely in morphology and noise. Relying on SMOTE alone can lead to a model that performs well on the training data (or even MIT-BIH test splits) but fails to detect unusual tachycardia presentations in new patients.

 

Consequences: Despite oversampling, sensitivity (recall) on tachycardia events likely remains suboptimal ‚Äì the model may still miss a significant portion of true tachy episodes. At the same time, oversampling can artificially boost performance metrics during cross-validation without genuinely improving generalization
ojs.stanford.edu
. In practice, detection of rare arrhythmias demands either more real data or domain-specific augmentation (e.g. adding noise, slight morphology variations) rather than synthetic interpolation alone. SMOTE alone is insufficient, and can even be counterproductive if it introduces borderline synthetic examples that confuse the model.

Labeling Quality vs. Clinical Definitions

Current Labeling Approach: It‚Äôs unclear how tachycardia events are labeled in the current pipeline ‚Äì whether based on MIT-BIH‚Äôs annotated beat types or custom definitions (e.g. any window with heart rate > 100 BPM tagged as tachycardia). This raises concern about label quality and clinical alignment. In clinical cardiology, tachycardia isn‚Äôt just any brief heart rate spike; it has specific definitions:

Sinus Tachycardia generally refers to a sustained heart rate >100 beats/min at rest
heart.org
 with normal rhythm origin (often a benign response to exercise or stress).

Ventricular Tachycardia (VT) is typically defined as three or more consecutive ventricular beats at >100 bpm. VT is further classified as non-sustained (lasting <30s) or sustained (>30s or requiring intervention)
droracle.ai
droracle.ai
. This is a clinically important distinction: even a 3-beat run of VT is significant, while sinus tachycardia requires sustained high rate.

Misalignment Risks: If the system‚Äôs labels for ‚Äútachycardia‚Äù do not exactly match these clinically validated criteria, the model may be learning a concept of tachycardia that doctors wouldn‚Äôt agree with. For example, labeling every 5-second window with HR >100 as tachycardia could mark transient accelerations or noise as events, whereas a cardiologist might ignore those. Conversely, if the labeling is too narrow (e.g. only tagging very high rates or certain arrhythmia codes), the model might overlook clinically relevant tachy episodes (such as a non-sustained 3-beat VT run at 110 bpm, which should be caught
droracle.ai
). In the MIT-BIH data, true ventricular tachycardia episodes are rare ‚Äì only a couple of records contain VT, and even those are unusual types (e.g. slow VT at ~105 bpm
physionet.org
). If the labeling did not capture these correctly or if it conflated different tachyarrhythmias, the ground truth is questionable.

 

Recommendation: The labeling process needs a reality check against clinical standards. It should be verified that tachycardia labels correspond to meaningful clinical events:

Align threshold-based labels with accepted definitions (e.g. require a minimum run length for VT, sustained duration for sinus tachycardia).

If using MIT-BIH‚Äôs expert annotations, ensure you map the arrhythmia codes (like Ventricular Flutter/Fibrillation, Ventricular Tachycardia, Supraventricular Tachy) properly to the ‚Äútachycardia‚Äù category. Any label noise or inconsistency here will directly limit model performance ceiling.

Dataset Limitations (48 Patients, Limited Distribution)

Small Sample Size: The MIT-BIH Arrhythmia Database contains only 48 records from 47 patients, collected in the 1980s. This lack of diversity severely restricts the system‚Äôs ability to generalize
ojs.stanford.edu
. The patients are a limited demographic, and the recordings primarily capture certain arrhythmias chosen for that era‚Äôs research. As a result:

The model might be ‚Äúlearning‚Äù patient-specific quirks or noise patterns from this small group, which will not hold in a broader population.

It likely has not seen a wide variety of tachycardia manifestations (different QRS morphologies, rates, and noise conditions) that occur in larger cohorts.

Rhythm Distribution: MIT-BIH was designed to overrepresent some arrhythmias (to have enough examples) ‚Äì e.g. record 203 has many premature ventricular beats, record 207 has episodes of VT, etc. But many arrhythmia types or conditions are absent (e.g. atrial tachyarrhythmias, certain supraventricular tachycardias). Thus, the training data is not representative of all tachycardia types. A model might become very good at detecting the few types present (say, one form of VT or frequent PVCs if those were labeled as tachy surrogates) and still miss others. Real-world ECGs can include noise, artifacts, pacemaker rhythms, or combinations of arrhythmias ‚Äì scenarios underrepresented in MIT-BIH.

 

Implication: A model trained solely on this dataset might show high accuracy in cross-validation, but face significant performance degradation on more variable external data
arxiv.org
. Indeed, studies have found that arrhythmia models trained on MIT-BIH with >95% accuracy can drop sharply when tested on a different database or on a combined dataset
arxiv.org
. This underscores that current performance metrics may be overly optimistic.

Sensitivity and Recall Gaps

Observed Recall Gaps: Given the above issues (imbalance and limited data), it‚Äôs highly likely the model exhibits recall gaps ‚Äì failing to catch some tachycardia events. If the model was optimized for overall accuracy or even for F1-score, it might be trading off recall to achieve higher precision or avoid false alarms. In a clinical context, missing a tachycardia is dangerous (false negatives can mean missing a life-threatening arrhythmia). The current system must be scrutinized for how many tachy episodes in the test set were missed. Even if the overall sensitivity on paper seems decent, one must ask: Were all the clinically significant tachycardias detected? Any gap here is a show-stopper for deployment.

 

Threshold Tuning: It‚Äôs possible the model‚Äôs threshold or decision criteria were not tuned for maximum sensitivity. Sometimes model developers use default 0.5 probability cutoff, which might not be appropriate when the positive class is rare and critical. By prioritizing sensitivity (recall) of tachycardia, one typically must lower the threshold ‚Äì but that then increases false positives. If the current system hasn‚Äôt explicitly managed this balance, it might either be too conservative (missing events to avoid false alarms) or too trigger-happy (detecting many false tachy episodes). Neither extreme is ideal; a nuanced threshold optimization is needed. We discuss false positives further below.

Model Training and Evaluation Issues
Model Choice: Traditional ML vs. CNN vs. LSTM

Current Approach: The question implies a consideration between traditional ML models and deep learning (CNN/LSTM). If the current system uses a traditional machine learning model (e.g. Random Forest, SVM) on engineered features, it might be inherently limited in capturing the full complexity of ECG signals:

Traditional models rely on human-defined features (like average heart rate, RR-interval variability, etc.). These may not fully capture subtle morphological clues of pathological tachycardia versus benign fast rhythms.

However, traditional models are simpler and less prone to overfitting on small data. With only 47 patients, a complex deep CNN could memorize noise unless carefully regularized.

If the system uses a Convolutional Neural Network (CNN) on raw or processed ECG segments, it likely excels at learning local patterns (e.g. shape of QRS complexes, waveform differences)
arxiv.org
. CNNs have achieved high accuracy in arrhythmia classification research
arxiv.org
. But CNNs have a fixed input window; if the window is short (say a few beats long), the model might miss longer-term context
arxiv.org
. Tachycardia detection may require recognizing a sustained sequence of fast beats, not just one abnormal beat. A CNN looking at 1-2 heartbeats might struggle with episodes where rate and rhythm context over several seconds matters.

 

Long Short-Term Memory networks (LSTMs) or other RNNs can capture temporal dependencies over longer periods
arxiv.org
, which is valuable for detecting a continuous tachycardic run. For example, an LSTM could theoretically detect an accelerating heart rate trend or ensure that ‚Äúthree or more consecutive beats‚Äù criterion is met by learning sequential patterns. However, RNNs are harder to train on limited data and can be less interpretable (some XAI methods struggle with recurrent architectures
arxiv.org
). If an LSTM was attempted and not properly regularized, it could overfit the 48 recordings or be unstable.

 

Shortcoming: The current model choice may not optimally balance these considerations. If a simpler model is used, it might not generalize to unseen morphologies; if a deep model is used, it might not generalize due to data scarcity. Additionally, a single-model approach might not suffice ‚Äì e.g., a pure CNN might flag individual beats as tachycardic, but without an algorithm to group them into an episode, it could either miss sustained events or raise too many alarms on sporadic fast beats.

Handling of Sequence and Context

Real-world tachycardia detection benefits from sequence context. For instance, a run of 5 slightly irregular fast beats could indicate atrial fibrillation onset or non-sustained VT. A model looking at one beat at a time (as many beat classifiers do with MIT-BIH) would lose that context. If the current system treats each heartbeat or short segment independently, this is a limitation. It might identify fast beats but not realize they form a prolonged tachycardia episode versus just an isolated ectopic beat. The mention of ‚ÄúXAI tachycardia detection‚Äù suggests the system might highlight regions of the ECG; ensuring those regions are considered in context (e.g. contiguous segment analysis) is important. Without context modeling, sensitivity can drop for sustained events and false positives can rise (e.g. labeling a single premature beat as ‚Äútachycardia‚Äù incorrectly).

Model Evaluation Methodology Flaws

Proper evaluation is crucial for a trustworthy model. Several methodological issues might be present:

Train/Test Split Strategy: If the data splitting was done at the beat level or randomly across the entire dataset, it would lead to leakage of patient-specific patterns into both train and test sets. MIT-BIH has long recordings; splitting without regard to record identity means the model could effectively be tested on data from patients it has already seen in training. This can grossly inflate performance metrics (the model recognizes person-specific ECG idiosyncrasies). The well-recognized best practice is to do an inter-patient split ‚Äì ensuring no patient‚Äôs data appears in both train and test
vuir.vu.edu.au
vuir.vu.edu.au
. If the current project did not follow this, its reported accuracy or AUC is likely over-optimistic and not indicative of true generalization.

Cross-Validation and Test Set Size: With only 47 patients, using a single train/test split might be brittle ‚Äì results can vary greatly depending on which few patients are held out. If they used, say, 80/20 split, the test set might be as small as 9 or 10 records, which may not reliably represent all tachyarrhythmia scenarios. Ideally, repeated cross-validation or using standard splits (like the AAMI EC57 recommended split) should be done to get robust estimates. It‚Äôs not clear if that was done. A failure to do patient-level cross-validation would mean the performance could drop when a different set of patients is tested.

Metrics Selection: The system‚Äôs evaluation might be focused on overall accuracy or the F1-score. However, for a highly imbalanced problem with clinical implications, these metrics can be insufficient. Accuracy is misleading (a trivial classifier that always says ‚Äúno tachycardia‚Äù could be >90% accurate on MIT-BIH because tachy events are rare). Even F1-score, which balances precision and recall, might not capture the operational needs. Clinically, sensitivity (recall) for tachycardia and the false positive rate are key. We care about ‚ÄúDid we catch all true events?‚Äù and ‚ÄúHow often are we crying wolf with false alarms?‚Äù. If the current evaluation didn‚Äôt explicitly report sensitivity (recall) and specificity or false alarm rates, it‚Äôs hard to judge clinical readiness. Moreover, positive predictive value (PPV) is crucial in practice ‚Äì a low PPV means many alarms are false, contributing to alarm fatigue.

Threshold and Operating Point: Was a decision threshold optimized or just defaulted? A ROC or Precision-Recall curve analysis is needed to choose a threshold that meets clinical requirements (e.g. maybe aiming for ‚â•95% sensitivity and then noting what false-positive rate that yields). The lack of mention of such threshold tuning suggests it might not have been thoroughly done, which can leave sensitivity gaps or an impractical false alarm rate.

Leakage Through Preprocessing: Another subtle risk is data leakage via preprocessing steps. For example, if signal normalization (scaling) was done using global data statistics including the test set, that would leak information. Or if SMOTE was applied before splitting data, synthetic points could effectively give the model clues about test distribution
vuir.vu.edu.au
. All augmentation oversampling must be confined to the training fold only. It‚Äôs worth reviewing the pipeline to ensure no such leakage occurred. Any leakage can make performance look better than it truly is.

In summary, the current evaluation may not reflect real-world performance due to potential patient overlap in splits and choice of metrics. A rigorous, leakage-free evaluation protocol focusing on clinically relevant metrics is needed.

Sensitivity vs. False Positives Trade-off

A critical aspect of evaluation (and ultimately deployment) is balancing sensitivity and false positive rate. The current model‚Äôs sensitivity for tachycardia might be lower than desired, as noted, but one cannot simply push it to 100% without considering false alarms. In clinical monitors, alarm fatigue is a huge problem ‚Äì studies show the majority of arrhythmia alarms can be false
heartrhythmopen.com
. For instance, a recent evaluation of hospital ECG monitors showed even the best system achieved 98% sensitivity for ventricular tachycardia but still had some false alarms, whereas other systems had sensitivities ~75‚Äì84% and hundreds of false alarms in just 205 hours of testing
pmc.ncbi.nlm.nih.gov
pmc.ncbi.nlm.nih.gov
. This underscores that excessive false positives can render a detection system unusable, no matter how high the sensitivity.

 

If the current model was trained to maximize F1 or accuracy, it might not be explicitly controlling the false positive rate. A tachycardia detector that fires off too often on benign fast heart rates or noise will not be clinically acceptable. Thus, a shortcoming in the current setup is likely the lack of a clear strategy to maintain high sensitivity and keep false alarms to a manageable level. This requires careful threshold setting, possibly multi-stage logic (to confirm alarms), and testing under noise. It‚Äôs not evident such steps have been taken yet.

Explainability (XAI) Concerns
XAI Tooling Gaps

The system is described as an ‚ÄúXAI tachycardia detection system,‚Äù implying it has some explainability component ‚Äì perhaps highlighting which part of the ECG led to the tachycardia prediction. While this is a great goal (clinicians are more likely to trust a model that can indicate why it‚Äôs sounding an alarm), there are gaps to address:

Choice of XAI Methods: Not all explainability techniques work well for ECG or time-series. If the project uses generic methods like SHAP or LIME out-of-the-box on a deep model, the results might be cluttered or hard to interpret on a waveform
arxiv.org
. For instance, KernelSHAP on a sequence model can highlight dozens of tiny features across the signal, which doesn‚Äôt map cleanly to clinical features
arxiv.org
. Clinicians have indicated a preference for saliency map-style explanations (highlighting waveform regions) over abstract feature importances
arxiv.org
, since it corresponds to how they read ECGs. If the current XAI output is not presented in a intuitive way (e.g. marking the segment where tachycardia was detected on the ECG trace), it may not actually be useful to end-users.

Potential Misleading Highlights: Even with saliency methods like Grad-CAM or integrated gradients, there is a risk that the model is attending to spurious patterns. For example, a model might learn to detect tachycardia by looking at an artifact or a segment of flatline preceding the tachy onset (if such patterns exist in training data). The XAI might then dutifully highlight those segments, misleading the practitioner into thinking those irrelevant segments are important. In ECG deep learning literature, it‚Äôs noted that without careful design, explainability methods can sometimes highlight the QRS complexes or other obvious elements without truly explaining the arrhythmia decision. If the current system hasn‚Äôt been validated for correctness of its explanations, the XAI component might give a false sense of security. Explanations must be taken with caution and ideally reviewed by experts: do the highlighted regions correspond to the fast heart rate or ectopic beats that define tachycardia, or are they off-target?

Lack of Quantitative XAI Evaluation: A shortcoming is that there is usually no quantitative measure provided for how ‚Äúexplainable‚Äù or trustworthy the explanations are. For clinical AI, it‚Äôs not enough to produce a pretty visualization; one needs to ensure the explanations are consistent and align with domain knowledge. For example, if multiple XAI techniques (say SHAP vs. gradient-based) give very different explanations, that‚Äôs a red flag. Ensuring stability of explanations across samples is important
arxiv.org
. The current system likely hasn‚Äôt undergone this level of scrutiny yet.

Integration into Workflow: Another gap is considering how the explainability will be used in practice. If a clinician gets an alarm with an highlighted ECG segment, is that sufficient for them to trust it? Possibly, but only if it‚Äôs displaying something they agree is concerning (e.g. a run of ventricular beats). The system should avoid information overload ‚Äì e.g. marking too many disparate points on the ECG could confuse users. There‚Äôs no indication that the current XAI approach has been tested with user feedback (which, ideally, it should eventually, as suggested by some studies 
arxiv.org
).

Summary of XAI Shortcomings: The concept is promising, but presently the explainability is likely unvalidated and possibly rudimentary. Without careful tuning, the XAI component might not mitigate the ‚Äúblack-box‚Äù concern sufficiently. This is significant because lack of interpretability is a major barrier to clinical AI adoption
ojs.stanford.edu
. Clinicians will require clear reasoning for alarms, and regulators often expect transparency for high-risk AI. The current system needs improvement here to be truly ready for clinical use.

Recommendations for Improvement

To move toward a clinically viable tachycardia detection system, several improvements are recommended. These address generalizability, sensitivity vs. false positives, evaluation rigor, and interpretability.

1. Data Augmentation and Expansion

Incorporate Diverse Data: Relying on MIT-BIH alone is not enough. Whenever possible, incorporate additional ECG datasets (e.g. other PhysioNet databases or private data) covering more patients and rhythm types
ojs.stanford.edu
. For example, data from wearable devices or other arrhythmia databases (AFDB, VT/VF databases, etc.) could improve diversity. If patient privacy or labeling is an issue, consider at least using some unlabeled ECG data to pre-train a model (unsupervised or self-supervised learning) to learn general ECG patterns, then fine-tune on tachycardia labels. The key is to expose the model to more variation. This directly enhances generalizability
ojs.stanford.edu
.

Physiologically Plausible Augmentation: In addition to SMOTE, use augmentation techniques that make sense for ECGs. For instance, slightly jittering the heart rate (time-scaling segments), adding baseline wander or Gaussian noise, random small shifts in QRS amplitude, etc. These augmentations create new training examples that reflect real-world signal noise and variability. They should be applied only on the training set and in moderation to avoid unrealistic data. Unlike SMOTE‚Äôs linear interpolation, these preserve the time-series nature and can help the model become robust to common signal variations.

Class Rebalancing Strategies: Instead of exclusively oversampling with SMOTE, consider a combination of approaches:

Selective Undersampling: Reduce the weight or frequency of normal rhythm examples (especially ones very similar to each other) so that the model doesn‚Äôt become biased towards the majority.

Cost-Sensitive Learning: Use class weights in the loss function to make the model pay more attention to tachycardia instances. This can often achieve better recall without synthetic data.

Data Synthesis with Caution: If generating synthetic tachycardia episodes, ensure they are reviewed or generated via a realistic model (e.g. using a generative adversarial network trained on ECGs, as some research explores, or by splicing real tachy segments into different backgrounds). The goal is to introduce realistic but novel examples of tachycardia. Always validate that synthetic data doesn‚Äôt introduce labeling errors or unrealistic artifacts.

Validate Label Alignment: Before retraining, refine the labeling of tachycardia:

Implement the clinically accepted criteria in code to label episodes. For example, scan the annotated beat sequence to find runs of ‚â•3 ventricular beats at >100 bpm and label those as VT episodes. Ensure sustained sinus tachycardia is labeled only if the rate is >100 for a prolonged period (e.g. >30 seconds continuously).

Consult a cardiologist or reference guidelines to confirm the labeling scheme. This will not only improve training signal for the model but also yield evaluation metrics that have clinical meaning (e.g. episode-level sensitivity: did we detect each VT run?).

If some labels in the dataset are noisy or ambiguous, consider cleaning them (even if that means discarding a few uncertain events) to avoid confusing the model.

By enriching and cleaning the data in these ways, the model will have a stronger foundation and be less likely to latch onto spurious correlations that won‚Äôt hold in new data.

2. Model Architecture and Training Enhancements

Use Hybrid Models for Sequence Learning: Given the need to capture both morphological details and temporal context, a hybrid approach is recommended. For example, a CNN + LSTM architecture can be effective: the CNN layers can learn features from short windows of ECG (detect QRS complexes, etc.), and then an LSTM (or GRU) can take the sequence of these features to determine if a tachycardia rhythm is present over time
ojs.stanford.edu
arxiv.org
. This way, the model can detect subtle beat-level abnormalities and understand the duration/succession that defines tachycardia. Such architectures have been reported to improve arrhythmia detection by capturing longer context without losing the efficiency of CNN feature extraction
arxiv.org
.

Regularization to Prevent Overfitting: With limited training data, any deep model must be heavily regularized. Employ techniques like dropout, L1/L2 regularization, and perhaps early stopping based on a validation set. Also, consider data augmentation (as above) as a form of regularization. If using an LSTM, apply recurrent dropout. Monitor for signs of overfitting (training vs. validation loss divergence) and constrain model complexity accordingly (e.g. limit number of layers or filters if needed).

Transfer Learning: As noted in research, transfer learning could boost performance when labeled data is scarce
ojs.stanford.edu
. For instance, one could pre-train a CNN on a large corpus of ECG signals (even from a different but related task, like detecting all arrhythmias or even on a different lead format) and then fine-tune on tachycardia detection. Pre-trained models might exist (some studies have trained deep networks on large ECG datasets like PhysioNet‚Äôs 2020 challenge data). This can improve feature robustness and generalization.

Alternative ML Approaches: If deep learning proves challenging with the data available, reconsider some traditional or hybrid approaches:

Use domain knowledge to extract additional features that signal tachycardia (e.g. heart rate trend features, presence of consecutive ectopic beats, etc.) and feed these into the model alongside raw data. A tree ensemble or even rule-based logic could combine with the ML outputs (for example, only flag tachycardia if the predicted probability is high and a certain heart rate threshold is exceeded for a minimum duration).

Anomaly detection approach: You could train a model on normal rhythm data (which is plentiful) and detect anomalies (which might include tachyarrhythmias) as deviations. However, given tachycardia is a known target, a supervised approach with the above improvements is likely better.

Ensemble Models: To improve reliability, consider an ensemble of models. Different algorithms (CNN, LSTM, random forest on features) could vote on tachycardia events. Ensemble averaging often improves generalization and can provide more stable performance. If one model is overly sensitive to noise, another might counteract it, and so on. This can also provide a form of uncertainty estimation: if only one out of three models is firing, maybe that‚Äôs a false alarm, whereas if all agree, it‚Äôs likely a true event.

Realtime Considerations (if relevant): While deployment specifics are out of scope, from a modeling perspective, ensure the architecture can eventually work in a streaming fashion (processing sequential data and detecting events as they happen). Models like LSTM naturally handle sequence streams, whereas large window CNNs might need to be applied in a rolling manner. This is just something to keep in mind when designing the solution, as it ties into how episodes are detected and terminated.

Overall, upgrading the model to effectively leverage temporal patterns and using techniques to combat overfitting will enhance its real-world performance. Any new model should always be evaluated with the rigorous methods below.

3. Robust Evaluation and Metrics

Inter-Patient Splits: Redesign the evaluation to use patient-separated splits exclusively. For example, use records 1-40 for training and 41-48 for testing (then possibly rotate in a cross-validation scheme). This ensures no leakage of person-specific features
vuir.vu.edu.au
. Many published works follow the AAMI EC57 standard partition or similar
vuir.vu.edu.au
 ‚Äì adopting such a standard split can also facilitate comparison with literature. If the current results were inflated by easier splitting, expect that more honest splitting will yield lower metrics ‚Äì but these will be more reflective of true performance that one can expect in new patients.

Use Clinically Meaningful Metrics: Shift the focus of evaluation to metrics that matter in a clinical setting:

Sensitivity (Recall) for Tachycardia: What fraction of true tachycardia episodes (or beats) are detected? This should be maximized, ideally above 90-95% for a serious consideration
pmc.ncbi.nlm.nih.gov
.

Specificity and False Alarm Rate: Instead of just overall specificity, compute the false positive rate per hour of ECG recording (e.g. how many false tachy alarms in 24 hours of monitoring). Clinicians will understand ‚Äú2 false alarms per hour‚Äù much more than an abstract specificity percentage. Aim to minimize this ‚Äì perhaps target <0.1 false alarms/hour as an aspirational goal based on the best monitors
pmc.ncbi.nlm.nih.gov
pmc.ncbi.nlm.nih.gov
. At least quantify it so improvements can be tracked.

Precision (Positive Predictive Value): The fraction of model-predicted tachy events that are real. Low precision means clinicians would see many alerts that are not truly tachycardia, undermining trust. This metric should be reported alongside sensitivity to ensure a balanced view.

F1-score or MCC: These can still be reported for academic completeness, but they are secondary. A high F1 might mean nothing if sensitivity is low but precision high, or vice versa, depending on what was prioritized. It‚Äôs better to explicitly see the trade-off.

ROC and PR Curves: Plot these curves to visualize performance across thresholds. Especially use Precision-Recall curve given the class imbalance. Determine an optimal operating point ‚Äì for instance, ‚Äúwe choose a threshold that gives 95% sensitivity, at which point precision is 85%, corresponding to ~1 false alarm per 10 hours‚Äù (just an example). This kind of reporting grounds the performance in practical terms.

Per-Record Analysis: Report performance per patient/record in test. This will show if the model fails completely on certain patients. For example, if in 8 test records the sensitivity was 100% on 7 of them but 0% on 1 of them, that‚Äôs important to know ‚Äì maybe that patient had an arrhythmia type the model doesn‚Äôt handle. It also reinforces generalizability (a robust model should perform consistently across varied patients, not just average out well).

Avoiding Overlap and Leakage: Double-check the entire data pipeline for leakage. Ensure that any preprocessing (filtering, normalization) uses parameters derived from training data only. If you did something like beat segmentation, make sure no info from test leaks via template selection, etc. If doing cross-validation, the splitting and any oversampling must be done within each fold only
vuir.vu.edu.au
. It‚Äôs easy to accidentally leak by oversampling before splitting or by reusing a normalization factor computed globally. A leak-proof evaluation is the only way to inspire confidence in results.

External Validation: If possible, test the trained model on an external dataset it has never seen. For example, there‚Äôs a MIT-BIH Supraventricular Arrhythmia database or others with tachyarrhythmias. Even a small-scale test on another dataset (without any training on it) will provide a sense of how well the model generalizes beyond MIT-BIH. A dramatic drop in performance on external data would confirm that more work is needed on generalization (and is not unusual ‚Äì many MIT-BIH-trained models suffer this
arxiv.org
). On the other hand, if it holds up reasonably, that‚Äôs a strong indicator of real-world promise. This kind of validation is often expected in clinical AI papers to avoid bias to a single dataset.

Statistical Rigor: Use confidence intervals for metrics, especially if test data is small. This quantifies uncertainty. Also consider statistical tests when comparing model versions (for instance, if you try a new architecture, show that any improvement in sensitivity is statistically significant and not just due to chance on a small test set). Given the high stakes, we want to be sure any claimed performance isn‚Äôt a fluke.

By adopting these evaluation practices, the development process will catch issues early (e.g. high false positives, or certain arrhythmias not detected) and provide a clear picture of whether the system is approaching clinical-quality performance.

4. Balancing Sensitivity and False Positives

Tune Decision Thresholds: As mentioned, determine the probability/output threshold that yields an acceptable sensitivity vs. precision trade-off. This might involve prioritizing near 100% sensitivity for life-threatening tachycardias (ventricular tachycardia/fibrillation) while tolerating a bit more false alarms for those, but being stricter on less critical tachy (maybe one could differentiate life-threatening arrhythmias vs mere sinus tachy). If the model doesn‚Äôt inherently distinguish subtypes, you still can set one threshold and then post-process alerts. The key is to explicitly choose the operating point rather than using the default. You may end up with, say, a threshold of 0.3 instead of 0.5 to catch more events ‚Äì and then you‚Äôll need measures to handle the extra false alerts.

False Alarm Reduction Techniques: To avoid excessive false positives without missing beats, consider implementing a secondary check or smoothing logic. Some ideas:

Require persistence of detection: e.g. only flag tachycardia if the model outputs ‚Äútachycardia‚Äù for several consecutive beats or a sliding window of a few seconds. This helps filter out one-off misfires. It aligns with the clinical definition that tachycardia is about a run of beats, not just one beat.

Incorporate a simple rule: e.g. if the model says tachycardia but the instantaneous heart rate calculated is below 100, it might be a false alarm (unless the model specifically detects something like VT with slightly lower rate, but generally tachy implies high rate). This could catch absurd errors.

Noise detection: The system could monitor signal quality and pause alerts during obviously noisy segments (many false monitor alarms are due to noise). If the input is a very noisy segment, perhaps hold off or lower confidence.

Use an outlier detector on model inputs: if an input ECG segment is very different from anything in training (out-of-distribution), the model‚Äôs prediction might be less trustworthy. Flagging or handling such cases (maybe defaulting to ‚Äúuncertain‚Äù rather than a tachy prediction) can prevent some false alarms. This is advanced, but worth noting for reliability.

Human-in-the-Loop failsafe: In design, consider that ultimately a clinician will verify alarms. The system should be tuned to assist them, not overwhelm them. It is often better to miss a few benign episodes if it means not crying wolf constantly ‚Äì except for truly life-threatening rhythms where sensitivity should be paramount. This balance could be achieved by categorizing tachycardia severity: for example, have a very sensitive detector for lethal arrhythmias (like sustained VT/VF) and a more specific detector for less dangerous tachy (sinus tachycardia alerts might not even be desirable in many scenarios because they are common and usually not immediately dangerous without other context). If the current system doesn‚Äôt differentiate, perhaps adding a simple heuristic to classify the type of tachycardia (based on rhythm morphology, e.g. wide QRS = likely VT) could allow differential handling.

Monitoring and Iteration: As you make these adjustments, continuously monitor how the sensitivity and false positive rates move. The relationship is often inverse; find the sweet spot. Use clinical input if available ‚Äì e.g., an cardiologist might say ‚ÄúI can tolerate about 1 false alarm per hour, but not 10; and I need at least 90% of VT detected.‚Äù Use such guidance to inform threshold and logic choices.

By explicitly addressing the false-positive problem, you‚Äôll improve the system‚Äôs precision and usability in practice, which is just as important as raw sensitivity.

5. Strengthening XAI and Interpretability

Adopt Domain-Specific XAI Techniques: Switch to or augment the explainability toolset with techniques known to work better for time-series/ECG. For instance:

Saliency maps (e.g. Grad-CAM, Integrated Gradients): These can highlight which time points in the ECG were most influential in the model‚Äôs tachycardia prediction. Ensure these are plotted back onto the ECG waveform, shading the region that contributed to the alarm. This aligns with clinician expectations (they expect to see the problematic rhythm segment highlighted).

Temporal Importance Curves: Provide a visualization of model output over time. For example, a rolling prediction probability that spikes during a tachy episode. This, combined with a marked threshold crossing point, can help explain that ‚Äúthe model‚Äôs confidence rose when the heart rate went high for a sustained period here.‚Äù

Feature-based explanations (if using feature engineering): If any physiological features are input (like avg RR, etc.), SHAP or feature importance could be shown in a small table to indicate what factors led to the classification. But keep this supplemental; doctors will primarily want to see the signal.

Improve Clarity of Explanations: If initial XAI results were ‚Äúcluttered‚Äù (e.g. highlighting many tiny regions)
arxiv.org
, consider smoothing them. For instance, use a sliding window and aggregate importance to contiguous segments. Focus on highlighting the core tachycardic segment (e.g. the run of fast beats). One approach is to post-process saliency by zeroing out anything below a certain percentile of importance so that only the top contributions (likely the tachy beats) are colored. The goal is a clear, simple explanation: e.g., ‚ÄúThese 5 seconds of the ECG (highlighted) caused the tachycardia alarm because the heart rate was high and beats were abnormal here.‚Äù

Validate Explanations with Experts: Before deploying, take some example detections (true positives and false positives) and show the explanations to a cardiologist or experienced clinician. Gather feedback: Did the explanation correspond to what they, as humans, see in the ECG? For true tachy events, ideally the highlighted region should correspond to the arrhythmia. For false alarms, if the XAI highlights noise or a benign variation, that gives insight into model flaws. Use this feedback to iterate:

If the model is highlighting wrong patterns, it might be focusing on the wrong features. That indicates a need to retrain the model with perhaps additional constraints or to feed it different features. For example, if it‚Äôs latching onto T-wave shape for tachycardia, maybe the labeling or training process taught it a spurious correlation.

If explanations are on point for true positives but not for false, that‚Äôs still useful: you might then engineer some rule to catch those false patterns.

Research into Self-Interpretable Models: In the long-term, you could explore models that inherently provide interpretation. For instance, attention-based models can sometimes highlight the influential time steps via attention weights. Or prototype-based models (learning prototypical ECG segments for each class) could show a ‚Äúnearest tachycardia prototype‚Äù to explain a decision. While not necessary for initial deployment, being aware of these can guide future development, as transparency will be crucial for regulatory approval and clinician trust
ojs.stanford.edu
.

Document the Decision Process: Make sure that for each alarm the system would raise, you can produce a concise explanation. This could be a combination of natural language and visual: e.g., ‚ÄúAlert: Ventricular Tachycardia detected ‚Äì explanation: 7 consecutive wide-complex beats at ~150 bpm were identified (see highlighted segment).‚Äù This kind of explanation, if validated, will greatly increase user confidence and provides accountability (one can trace why an alarm was triggered). It also helps in debugging the system by examining false alarm explanations.

By bolstering the XAI component in these ways, the system will not only detect tachycardia but also provide actionable insight, bridging the gap between algorithm and clinician. Remember that a black-box algorithm, no matter how accurate, is harder to trust in medicine
ojs.stanford.edu
. We want the model to be as transparent as possible about what it‚Äôs seeing ‚Äì without overwhelming the user with extraneous info.

6. Other Considerations

Although UI and deployment are not the focus, for clinical accuracy it‚Äôs worth noting:

Reproducibility: Ensure that the entire pipeline from data preprocessing to model inference is deterministic and well-documented. In real-world settings, reproducibility of results is key for debugging. For example, if the model is integrated into a bedside monitor, one must be able to trace back exactly what data it saw and how it made a decision. This might mean setting random seeds, version controlling the code, and logging intermediate steps.

Latency and Real-Time Behavior: A model that‚Äôs too slow could miss real-time detection of tachycardia (though offline analysis on a static dataset like MIT-BIH doesn‚Äôt test this). As you move to deployment, consider the computational load. CNN-LSTM models can be heavy; ensure optimizations or a plan for real-time processing if the aspiration is a live monitor.

Focus remains on accuracy and reliability improvements; however, these engineering details will eventually matter for translating the system to practice.

Prioritized Action Items

The following table summarizes concrete action items to address the identified shortcomings, prioritized by their impact on real-world clinical readiness:

Issue/Shortcoming	Recommended Action	Priority
Limited dataset (47 patients, low diversity)	- Augment with additional ECG datasets (more patients, arrhythmias)
- Apply realistic data augmentations (noise, scaling, etc.)
ojs.stanford.edu
ojs.stanford.edu

- Pre-train or use transfer learning on larger ECG data
ojs.stanford.edu
	High üü¢
Class imbalance (rare tachy events)	- Use class weighting in loss to emphasize tachycardia
- Generate physiologically realistic synthetic data (cautiously) instead of plain SMOTE
ojs.stanford.edu

- Undersample or down-weight excess normal beats to balance training	High üü¢
Labeling misalignment with clinical tachycardia	- Redefine labels using clinical criteria (e.g. VT = ‚â•3 beats >100 bpm)
droracle.ai

- Verify and clean labels with clinical input (ensure true tachy episodes are correctly tagged)
- Separate different tachycardia types if needed (so model can learn nuances)	Medium üü°
Suboptimal model architecture for sequences	- Implement CNN+LSTM (or similar) to capture temporal context
arxiv.org
arxiv.org

- Regularize heavily (dropout, etc.) to prevent overfitting on small data
- Consider ensemble of models to improve robustness	High üü¢
Sensitivity (recall) gaps	- Lower decision threshold or adjust decision rules to catch more true events
- Emphasize recall in training (e.g. use recall-oriented loss or metric for early stopping)
- Post-process model outputs to require sustained detection (to truly capture episodes)	High üü¢
High false positive risk	- Implement false-alarm reduction: e.g. require consecutive positive outputs before alarm
- Tune threshold to meet target false alarm rate (while preserving sensitivity)
- Filter out detections during noisy segments or when HR is not actually elevated	High üü¢
Evaluation methodology issues	- Use only inter-patient train/test splits to avoid leakage
vuir.vu.edu.au

- Perform k-fold cross-validation at patient level for robust estimates
- Adopt clinically relevant metrics: sensitivity, false alarm/hour, PPV, etc., for evaluation
- Conduct an external validation test on a different dataset to gauge generalization
arxiv.org
	High üü¢
Explainability (XAI) shortcomings	- Switch to saliency map-based explanations for clarity
arxiv.org
arxiv.org

- Simplify and highlight the key segment that triggered detection (make it visual on the ECG)
- Validate explanations with experts; retrain or adjust if explanations don‚Äôt align with clinical reasoning
ojs.stanford.edu

- Ensure XAI tools are integrated into the pipeline and produce output for each alert	Medium üü°
Lack of clinician trust/interpretability	- Provide clear, case-specific reasoning for each alarm (e.g. ‚ÄúX consecutive fast beats detected‚Äù)
- Log model confidence and features for each decision for auditability
- Explore interpretable model designs (attention mechanisms, prototype learning) in the long term
ojs.stanford.edu
	Medium üü°
Reproducibility & Reliability	- Document and fix random seeds for training for consistent results
- Use version control for data preprocessing and model code
- Design the pipeline for real-time streaming compatibility (buffering data, etc.) to ensure no accuracy loss in live use	Low üîµ (later)

Legend: üü¢ High priority ‚Äì critical for real-world readiness; üü° Medium ‚Äì important improvement but perhaps iterative; üîµ Low ‚Äì nice-to-have or for future once essentials are in place.

Conclusion

In its current state, the tachycardia detection system demonstrates a solid starting point but is not yet ready for real-world clinical deployment. Key deficits include insufficient sensitivity to all clinically relevant tachyarrhythmias, over-reliance on a small imbalanced dataset, and potential overestimation of performance due to methodological issues. The explainability, while a strength of the project‚Äôs vision, needs refinement to truly aid clinical decision-making and avoid misinterpretation.

 

By systematically addressing the data pipeline, model architecture, evaluation protocol, and XAI presentation as detailed above, the system can be significantly strengthened. The focus should be on robust generalization and reliability: ensuring the model performs well not just on MIT-BIH but on varied real-world data, and that it does so with minimal missed events and an acceptable false alarm rate. Equally important is maintaining clinician trust through transparent explanations and alignment with medical knowledge.

 

Implementing the recommended changes (starting with the high-priority items) will bring the project closer to a clinically viable tool. This process will likely be iterative ‚Äì improvements in one area (say, more data or a better model) will necessitate re-evaluation of metrics and thresholds, and so on. In the end, success will be measured by a model that can consistently detect true tachycardia episodes with high sensitivity, while keeping false alarms low, and can explain its findings in a manner that clinicians find credible and useful. With those pieces in place, the system would be on a strong footing for real-world use in tachycardia detection and patient monitoring.